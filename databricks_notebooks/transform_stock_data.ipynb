{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f33ba216-5d94-4eed-afeb-cc36c329070d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Amazon S3 Integration\n",
    "\n",
    "Read json files from S3, process them one by one, then save them back to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "71497e5b-fcba-4b2e-a53a-ebfa96b75d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set AWS Credentials in Spark\n",
    "# aws_access_key = dbutils.secrets.get(scope=\"aws-credentials\", key=\"aws-access-key\")\n",
    "# aws_secret_key = dbutils.secrets.get(scope=\"aws-credentials\", key=\"aws-secret-key\")\n",
    "# spark.conf.set(\"fs.s3a.access.key\", aws_access_key)\n",
    "# spark.conf.set(\"fs.s3a.secret.key\", aws_secret_key)\n",
    "# spark.conf.set(\"fs.s3a.endpoint\", \"s3.ap-southeast-2.amazonaws.com\")  # Sydney region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9846f94b-c18c-48e2-86ac-2d06a3b3d9c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mount S3 bucket to DBFS\n",
    "\n",
    "# aws_access_key = dbutils.secrets.get(scope=\"aws-credentials\", key=\"aws-access-key\")\n",
    "# aws_secret_key = dbutils.secrets.get(scope=\"aws-credentials\", key=\"aws-secret-key\")\n",
    "# bucket_name = \"ernest-aws-bucket\"\n",
    "# mount_point = \"/mnt/stock-data\"\n",
    "# try:\n",
    "#     dbutils.fs.mount(\n",
    "#         source=f\"s3a://{bucket_name}\",\n",
    "#         mount_point=mount_point,\n",
    "#         extra_configs={\n",
    "#             \"fs.s3a.access.key\": aws_access_key,\n",
    "#             \"fs.s3a.secret.key\": aws_secret_key,\n",
    "#             \"fs.s3a.endpoint\": \"s3.ap-southeast-2.amazonaws.com\"\n",
    "#         }\n",
    "#     )\n",
    "#     print(f\"Mounted {bucket_name} to {mount_point}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Mount failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfc08b7c-da46-4945-a0e7-db4de0a2cac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found JSON files: ['s3://ernest-aws-bucket/raw/data/AAPL_raw_20250324_004550.json', 's3://ernest-aws-bucket/raw/data/GOOGL_raw_20250324_004551.json', 's3://ernest-aws-bucket/raw/data/MSFT_raw_20250324_004552.json', 's3://ernest-aws-bucket/raw/data/TSLA_raw_20250324_004553.json']\n"
     ]
    }
   ],
   "source": [
    "# Access S3 via Catalog\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# Initialise Spark session\n",
    "spark = SparkSession.builder.appName(\"StockDataTransform\").getOrCreate()\n",
    "\n",
    "# Define S3 path via external location\n",
    "bucket_name = \"ernest-aws-bucket\"\n",
    "s3_raw_data_path = f\"s3://{bucket_name}/raw/data\"\n",
    "s3_archive_path = f\"s3://{bucket_name}/raw/archive\"\n",
    "\n",
    "# List files in S3 raw/data/\n",
    "file_list = dbutils.fs.ls(s3_raw_data_path)\n",
    "json_files = [f.path for f in file_list if f.name.endswith(\".json\")]\n",
    "print(\"Found JSON files:\", json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac8a585d-1175-4919-a7a3-03b11df26a6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AAPL\nLast refreshed: 2025-03-21\nSaved AAPL data to s3://ernest-aws-bucket/processed/data/AAPL.parquet\n+------------+----------+-------+-------+------+------+--------+-----------+\n|stock_symbol|date      |open   |high   |low   |close |volume  |daily_range|\n+------------+----------+-------+-------+------+------+--------+-----------+\n|AAPL        |2024-10-25|229.74 |233.22 |229.57|231.41|38802304|3.649994   |\n|AAPL        |2024-10-28|233.32 |234.73 |232.55|233.4 |36087134|2.1799927  |\n|AAPL        |2024-10-29|233.1  |234.325|232.32|233.67|35417247|2.0049896  |\n|AAPL        |2024-10-31|229.34 |229.83 |225.37|225.91|64370086|4.4600067  |\n|AAPL        |2024-11-01|220.965|225.35 |220.27|222.91|65276741|5.080002   |\n+------------+----------+-------+-------+------+------+--------+-----------+\nonly showing top 5 rows\nMoved s3://ernest-aws-bucket/raw/data/AAPL_raw_20250324_004550.json to s3://ernest-aws-bucket/raw/archive/AAPL_raw_20250324_004550.json\nProcessing GOOGL\nLast refreshed: 2025-03-21\nSaved GOOGL data to s3://ernest-aws-bucket/processed/data/GOOGL.parquet\n+------------+----------+------+------+--------+------+--------+-----------+\n|stock_symbol|date      |open  |high  |low     |close |volume  |daily_range|\n+------------+----------+------+------+--------+------+--------+-----------+\n|GOOGL       |2024-10-25|163.67|165.59|163.42  |165.27|19828884|2.1699982  |\n|GOOGL       |2024-10-28|168.75|168.75|163.95  |166.72|32138641|4.800003   |\n|GOOGL       |2024-10-29|167.73|170.38|167.0896|169.68|42169025|3.2904053  |\n|GOOGL       |2024-10-30|180.68|182.02|174.0599|174.46|68890787|7.9600983  |\n|GOOGL       |2024-10-31|173.13|176.82|171.0   |171.11|44768981|5.8200073  |\n+------------+----------+------+------+--------+------+--------+-----------+\nonly showing top 5 rows\nMoved s3://ernest-aws-bucket/raw/data/GOOGL_raw_20250324_004551.json to s3://ernest-aws-bucket/raw/archive/GOOGL_raw_20250324_004551.json\nProcessing MSFT\nLast refreshed: 2025-03-21\nSaved MSFT data to s3://ernest-aws-bucket/processed/data/MSFT.parquet\n+------------+----------+-------+------+--------+------+--------+-----------+\n|stock_symbol|date      |open   |high  |low     |close |volume  |daily_range|\n+------------+----------+-------+------+--------+------+--------+-----------+\n|MSFT        |2024-10-25|426.76 |432.52|426.565 |428.15|16899064|5.9549866  |\n|MSFT        |2024-10-29|428.0  |433.17|425.8001|431.95|17644080|7.3699036  |\n|MSFT        |2024-10-30|437.435|438.5 |432.1   |432.53|29749149|6.399994   |\n|MSFT        |2024-11-01|409.01 |415.5 |407.5   |410.37|24230442|8.0        |\n|MSFT        |2024-11-06|412.42 |420.45|410.52  |420.18|26681842|9.930023   |\n+------------+----------+-------+------+--------+------+--------+-----------+\nonly showing top 5 rows\nMoved s3://ernest-aws-bucket/raw/data/MSFT_raw_20250324_004552.json to s3://ernest-aws-bucket/raw/archive/MSFT_raw_20250324_004552.json\nProcessing TSLA\nLast refreshed: 2025-03-21\nSaved TSLA data to s3://ernest-aws-bucket/processed/data/TSLA.parquet\n+------------+----------+-------+-------+------+------+---------+-----------+\n|stock_symbol|date      |open   |high   |low   |close |volume   |daily_range|\n+------------+----------+-------+-------+------+------+---------+-----------+\n|TSLA        |2024-10-25|256.01 |269.49 |255.32|269.19|161611931|14.169983  |\n|TSLA        |2024-10-28|270.0  |273.536|262.24|262.51|107653603|11.2960205 |\n|TSLA        |2024-10-29|264.51 |264.98 |255.51|259.52|80521751 |9.4700165  |\n|TSLA        |2024-10-31|257.99 |259.75 |249.25|249.85|66575292 |10.5       |\n|TSLA        |2024-11-01|252.043|254.0  |246.63|248.98|57544757 |7.369995   |\n+------------+----------+-------+-------+------+------+---------+-----------+\nonly showing top 5 rows\nMoved s3://ernest-aws-bucket/raw/data/TSLA_raw_20250324_004553.json to s3://ernest-aws-bucket/raw/archive/TSLA_raw_20250324_004553.json\nAll files processed.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import col, lit, input_file_name, regexp_extract\n",
    "\n",
    "# Process each file\n",
    "\n",
    "for json_file in json_files:\n",
    "    filename = json_file.split(\"/\")[-1]\n",
    "    raw_df = spark.read.json(json_file)\n",
    "    stock_symbol = raw_df.select(\"Meta Data.`2. Symbol`\").first()[0]\n",
    "    last_refreshed = raw_df.select(\"Meta Data.`3. Last Refreshed`\").first()[0]\n",
    "    print(f\"Processing {stock_symbol}\")\n",
    "    print(f\"Last refreshed: {last_refreshed}\")\n",
    "\n",
    "    # Check if last refreshed is within 3 days\n",
    "    current_date = datetime.now()\n",
    "    last_refreshed_date = datetime.strptime(last_refreshed, \"%Y-%m-%d\")\n",
    "    if current_date - last_refreshed_date > timedelta(days=3):\n",
    "        print(f\"Skipping {stock_symbol} as it was last refreshed on {last_refreshed}\")\n",
    "        # Move file to archive\n",
    "        try:\n",
    "            dbutils.fs.mv(json_file, f\"{s3_archive_path}/{filename}\")\n",
    "            print(f\"Moved {json_file} to {s3_archive_path}/{filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error moving {json_file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Flatten Time Series (Daily)\n",
    "    time_series_df = raw_df.select(\"Time Series (Daily).*\")\n",
    "    date_columns = time_series_df.columns\n",
    "    rows = [time_series_df.select(lit(date).alias(\"date\"), col(date).alias(\"values\")) for date in date_columns]\n",
    "    flattened_df = rows[0]\n",
    "    for row_df in rows[1:]:\n",
    "        flattened_df = flattened_df.union(row_df)\n",
    "\n",
    "    # Flatten values struct\n",
    "    clean_df = flattened_df.select(\n",
    "        lit(stock_symbol).alias(\"stock_symbol\"),\n",
    "        col(\"date\"),\n",
    "        col(\"values.`1. open`\").cast(\"float\").alias(\"open\"),\n",
    "        col(\"values.`2. high`\").cast(\"float\").alias(\"high\"),\n",
    "        col(\"values.`3. low`\").cast(\"float\").alias(\"low\"),\n",
    "        col(\"values.`4. close`\").cast(\"float\").alias(\"close\"),\n",
    "        col(\"values.`5. volume`\").cast(\"long\").alias(\"volume\")\n",
    "    )\n",
    "    \n",
    "    # Add transformations\n",
    "    clean_df = clean_df.withColumn(\"daily_range\", col(\"high\") - col(\"low\"))\n",
    "    # Save to S3\n",
    "    output_s3_path = f\"s3://{bucket_name}/processed/data/{stock_symbol}.parquet\"\n",
    "    clean_df.write.mode(\"overwrite\").parquet(output_s3_path)\n",
    "    print(f\"Saved {stock_symbol} data to {output_s3_path}\")\n",
    "\n",
    "    # Verify\n",
    "    verify_df = spark.read.parquet(output_s3_path)\n",
    "    verify_df.show(5, truncate=False)\n",
    "\n",
    "    # Move file to archive\n",
    "    dbutils.fs.mv(json_file, f\"{s3_archive_path}/{filename}\")\n",
    "    print(f\"Moved {json_file} to {s3_archive_path}/{filename}\")\n",
    "\n",
    "print(\"All files processed.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7513946077669852,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "s3_integration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}