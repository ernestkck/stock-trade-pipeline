{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "751b91e7-1440-4f65-837d-093cc8cd0e7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Amazon S3 Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fc21253-9c97-4d8c-8242-3721b8264e5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read JSON from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "71497e5b-fcba-4b2e-a53a-ebfa96b75d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-746201911757538>, line 4\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m aws_access_key \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39msecrets\u001B[38;5;241m.\u001B[39mget(scope\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maws-credentials\u001B[39m\u001B[38;5;124m\"\u001B[39m, key\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maws-access-key\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      3\u001B[0m aws_secret_key \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39msecrets\u001B[38;5;241m.\u001B[39mget(scope\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maws-credentials\u001B[39m\u001B[38;5;124m\"\u001B[39m, key\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maws-secret-key\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m----> 4\u001B[0m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.s3a.access.key\u001B[39m\u001B[38;5;124m\"\u001B[39m, aws_access_key)\n",
       "\u001B[1;32m      5\u001B[0m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.s3a.secret.key\u001B[39m\u001B[38;5;124m\"\u001B[39m, aws_secret_key)\n",
       "\u001B[1;32m      6\u001B[0m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.s3a.endpoint\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ms3.ap-southeast-2.amazonaws.com\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/conf.py:46\u001B[0m, in \u001B[0;36mRuntimeConf.set\u001B[0;34m(self, key, value)\u001B[0m\n",
       "\u001B[1;32m     44\u001B[0m op_set \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mSet(pairs\u001B[38;5;241m=\u001B[39m[proto\u001B[38;5;241m.\u001B[39mKeyValue(key\u001B[38;5;241m=\u001B[39mkey, value\u001B[38;5;241m=\u001B[39mvalue)])\n",
       "\u001B[1;32m     45\u001B[0m operation \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mOperation(\u001B[38;5;28mset\u001B[39m\u001B[38;5;241m=\u001B[39mop_set)\n",
       "\u001B[0;32m---> 46\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m(\u001B[49m\u001B[43moperation\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m warn \u001B[38;5;129;01min\u001B[39;00m result\u001B[38;5;241m.\u001B[39mwarnings:\n",
       "\u001B[1;32m     48\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(warn)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1726\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n",
       "\u001B[1;32m   1724\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   1725\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1726\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43merror\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1913\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   1911\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   1912\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 1913\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_rpc_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43merror\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1914\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
       "\u001B[1;32m   1915\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1988\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   1985\u001B[0m             info \u001B[38;5;241m=\u001B[39m error_details_pb2\u001B[38;5;241m.\u001B[39mErrorInfo()\n",
       "\u001B[1;32m   1986\u001B[0m             d\u001B[38;5;241m.\u001B[39mUnpack(info)\n",
       "\u001B[0;32m-> 1988\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   1989\u001B[0m                 info,\n",
       "\u001B[1;32m   1990\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   1991\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   1992\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   1993\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m   1995\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(status\u001B[38;5;241m.\u001B[39mmessage) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m   1996\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [CONFIG_NOT_AVAILABLE] Configuration fs.s3a.access.key is not available. SQLSTATE: 42K0I\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.AnalysisException\n",
       "\tat com.databricks.sql.connect.SparkConnectConfig$.assertConfigAllowed(SparkConnectConfig.scala:238)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler$RuntimeConfigWrapper.set(SparkConnectConfigHandler.scala:99)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handleSet$1(SparkConnectConfigHandler.scala:264)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.handleSet(SparkConnectConfigHandler.scala:261)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$doHandle$1(SparkConnectConfigHandler.scala:232)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$doHandle$1$adapted(SparkConnectConfigHandler.scala:211)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:404)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1426)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:404)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
       "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:403)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.doHandle(SparkConnectConfigHandler.scala:211)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handle$1(SparkConnectConfigHandler.scala:202)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handle$1$adapted(SparkConnectConfigHandler.scala:186)\n",
       "\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:258)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.handle(SparkConnectConfigHandler.scala:186)\n",
       "\tat org.apache.spark.sql.connect.service.SparkConnectService.config(SparkConnectService.scala:129)\n",
       "\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:805)\n",
       "\tat grpc_shaded.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n",
       "\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n",
       "\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n",
       "\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n",
       "\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:351)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:306)\n",
       "\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:500)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:306)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:229)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:305)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:318)\n",
       "\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:298)\n",
       "\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:351)\n",
       "\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n",
       "\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n",
       "\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n",
       "\tat grpc_shaded.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:351)\n",
       "\tat grpc_shaded.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n",
       "\tat grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n",
       "\tat grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
       "\tat java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[CONFIG_NOT_AVAILABLE] Configuration fs.s3a.access.key is not available. SQLSTATE: 42K0I\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat com.databricks.sql.connect.SparkConnectConfig$.assertConfigAllowed(SparkConnectConfig.scala:238)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler$RuntimeConfigWrapper.set(SparkConnectConfigHandler.scala:99)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handleSet$1(SparkConnectConfigHandler.scala:264)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.handleSet(SparkConnectConfigHandler.scala:261)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$doHandle$1(SparkConnectConfigHandler.scala:232)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$doHandle$1$adapted(SparkConnectConfigHandler.scala:211)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:404)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1426)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:404)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:403)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.doHandle(SparkConnectConfigHandler.scala:211)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handle$1(SparkConnectConfigHandler.scala:202)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handle$1$adapted(SparkConnectConfigHandler.scala:186)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:258)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.handle(SparkConnectConfigHandler.scala:186)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.config(SparkConnectService.scala:129)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:805)\n\tat grpc_shaded.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:351)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:306)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:500)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:306)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:229)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:305)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:318)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:298)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:351)\n\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat grpc_shaded.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:351)\n\tat grpc_shaded.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
       },
       "metadata": {
        "errorSummary": "[CONFIG_NOT_AVAILABLE] Configuration fs.s3a.access.key is not available. SQLSTATE: 42K0I"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "CONFIG_NOT_AVAILABLE",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "sqlState": "42K0I",
        "stackTrace": "org.apache.spark.sql.AnalysisException\n\tat com.databricks.sql.connect.SparkConnectConfig$.assertConfigAllowed(SparkConnectConfig.scala:238)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler$RuntimeConfigWrapper.set(SparkConnectConfigHandler.scala:99)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handleSet$1(SparkConnectConfigHandler.scala:264)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.handleSet(SparkConnectConfigHandler.scala:261)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$doHandle$1(SparkConnectConfigHandler.scala:232)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$doHandle$1$adapted(SparkConnectConfigHandler.scala:211)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:404)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1426)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:404)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:403)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.doHandle(SparkConnectConfigHandler.scala:211)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handle$1(SparkConnectConfigHandler.scala:202)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handle$1$adapted(SparkConnectConfigHandler.scala:186)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:258)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.handle(SparkConnectConfigHandler.scala:186)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.config(SparkConnectService.scala:129)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:805)\n\tat grpc_shaded.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:351)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:306)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:500)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:306)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:229)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:305)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:318)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:298)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:351)\n\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat grpc_shaded.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:351)\n\tat grpc_shaded.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-746201911757538>, line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m aws_access_key \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39msecrets\u001B[38;5;241m.\u001B[39mget(scope\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maws-credentials\u001B[39m\u001B[38;5;124m\"\u001B[39m, key\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maws-access-key\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m aws_secret_key \u001B[38;5;241m=\u001B[39m dbutils\u001B[38;5;241m.\u001B[39msecrets\u001B[38;5;241m.\u001B[39mget(scope\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maws-credentials\u001B[39m\u001B[38;5;124m\"\u001B[39m, key\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maws-secret-key\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.s3a.access.key\u001B[39m\u001B[38;5;124m\"\u001B[39m, aws_access_key)\n\u001B[1;32m      5\u001B[0m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.s3a.secret.key\u001B[39m\u001B[38;5;124m\"\u001B[39m, aws_secret_key)\n\u001B[1;32m      6\u001B[0m spark\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfs.s3a.endpoint\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ms3.ap-southeast-2.amazonaws.com\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/conf.py:46\u001B[0m, in \u001B[0;36mRuntimeConf.set\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m     44\u001B[0m op_set \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mSet(pairs\u001B[38;5;241m=\u001B[39m[proto\u001B[38;5;241m.\u001B[39mKeyValue(key\u001B[38;5;241m=\u001B[39mkey, value\u001B[38;5;241m=\u001B[39mvalue)])\n\u001B[1;32m     45\u001B[0m operation \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mOperation(\u001B[38;5;28mset\u001B[39m\u001B[38;5;241m=\u001B[39mop_set)\n\u001B[0;32m---> 46\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m(\u001B[49m\u001B[43moperation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m warn \u001B[38;5;129;01min\u001B[39;00m result\u001B[38;5;241m.\u001B[39mwarnings:\n\u001B[1;32m     48\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(warn)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1726\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n\u001B[1;32m   1724\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1725\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1726\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43merror\u001B[49m\u001B[43m)\u001B[49m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1913\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   1911\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1912\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 1913\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle_rpc_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43merror\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1914\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   1915\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/client/core.py:1988\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   1985\u001B[0m             info \u001B[38;5;241m=\u001B[39m error_details_pb2\u001B[38;5;241m.\u001B[39mErrorInfo()\n\u001B[1;32m   1986\u001B[0m             d\u001B[38;5;241m.\u001B[39mUnpack(info)\n\u001B[0;32m-> 1988\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   1989\u001B[0m                 info,\n\u001B[1;32m   1990\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   1991\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   1992\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   1993\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m   1995\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(status\u001B[38;5;241m.\u001B[39mmessage) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m   1996\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [CONFIG_NOT_AVAILABLE] Configuration fs.s3a.access.key is not available. SQLSTATE: 42K0I\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat com.databricks.sql.connect.SparkConnectConfig$.assertConfigAllowed(SparkConnectConfig.scala:238)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler$RuntimeConfigWrapper.set(SparkConnectConfigHandler.scala:99)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handleSet$1(SparkConnectConfigHandler.scala:264)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.handleSet(SparkConnectConfigHandler.scala:261)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$doHandle$1(SparkConnectConfigHandler.scala:232)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$doHandle$1$adapted(SparkConnectConfigHandler.scala:211)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:404)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1426)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:404)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:240)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:403)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.doHandle(SparkConnectConfigHandler.scala:211)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handle$1(SparkConnectConfigHandler.scala:202)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handle$1$adapted(SparkConnectConfigHandler.scala:186)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:258)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.handle(SparkConnectConfigHandler.scala:186)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.config(SparkConnectService.scala:129)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:805)\n\tat grpc_shaded.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:351)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:306)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:500)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:306)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:229)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:305)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:318)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:298)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:351)\n\tat grpc_shaded.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat grpc_shaded.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat grpc_shaded.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:351)\n\tat grpc_shaded.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat grpc_shaded.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat grpc_shaded.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set AWS Credentials in Spark\n",
    "aws_access_key = dbutils.secrets.get(scope=\"aws-credentials\", key=\"aws-access-key\")\n",
    "aws_secret_key = dbutils.secrets.get(scope=\"aws-credentials\", key=\"aws-secret-key\")\n",
    "spark.conf.set(\"fs.s3a.access.key\", aws_access_key)\n",
    "spark.conf.set(\"fs.s3a.secret.key\", aws_secret_key)\n",
    "spark.conf.set(\"fs.s3a.endpoint\", \"s3.ap-southeast-2.amazonaws.com\")  # Sydney region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9846f94b-c18c-48e2-86ac-2d06a3b3d9c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mount failed: An error occurred while calling o933.mount. Trace:\npy4j.security.Py4JSecurityException: Method public com.databricks.backend.daemon.dbutils.DBUtilsCore$Result com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.util.Map) is not whitelisted on class class com.databricks.backend.daemon.dbutils.DBUtilsCore\n\tat py4j.security.WhitelistingPy4JSecurityManager.checkCall(WhitelistingPy4JSecurityManager.java:473)\n\tat py4j.Gateway.invoke(Gateway.java:305)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\n"
     ]
    }
   ],
   "source": [
    "# Mount S3 bucket to DBFS\n",
    "\n",
    "aws_access_key = dbutils.secrets.get(scope=\"aws-credentials\", key=\"aws-access-key\")\n",
    "aws_secret_key = dbutils.secrets.get(scope=\"aws-credentials\", key=\"aws-secret-key\")\n",
    "bucket_name = \"ernest-aws-bucket\"\n",
    "mount_point = \"/mnt/stock-data\"\n",
    "try:\n",
    "    dbutils.fs.mount(\n",
    "        source=f\"s3a://{bucket_name}\",\n",
    "        mount_point=mount_point,\n",
    "        extra_configs={\n",
    "            \"fs.s3a.access.key\": aws_access_key,\n",
    "            \"fs.s3a.secret.key\": aws_secret_key,\n",
    "            \"fs.s3a.endpoint\": \"s3.ap-southeast-2.amazonaws.com\"\n",
    "        }\n",
    "    )\n",
    "    print(f\"Mounted {bucket_name} to {mount_point}\")\n",
    "except Exception as e:\n",
    "    print(f\"Mount failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfc08b7c-da46-4945-a0e7-db4de0a2cac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Access S3 via Catalog\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialise Spark session\n",
    "spark = SparkSession.builder.appName(\"StockDataTransform\").getOrCreate()\n",
    "\n",
    "# Define S3 path via external location\n",
    "bucket_name = \"ernest-aws-bucket\"\n",
    "raw_s3_path = f\"s3://{bucket_name}/raw/data/*.json\"\n",
    "\n",
    "# Read JSON files\n",
    "try:\n",
    "    raw_df = spark.read.json(raw_s3_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading from S3: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec206908-0fb4-41f7-aa37-3f4163dc9fd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Meta Data: struct (nullable = true)\n |    |-- 1. Information: string (nullable = true)\n |    |-- 2. Symbol: string (nullable = true)\n |    |-- 3. Last Refreshed: string (nullable = true)\n |    |-- 4. Output Size: string (nullable = true)\n |    |-- 5. Time Zone: string (nullable = true)\n |-- Time Series (Daily): struct (nullable = true)\n |    |-- 2024-10-18: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-10-21: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-10-22: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-10-23: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-10-24: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-10-25: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-10-28: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-10-29: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-10-30: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-10-31: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-11-01: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-11-04: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-11-05: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-11-06: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-11-07: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-11-08: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-11-11: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-11-12: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-11-13: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-11-14: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-11-15: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-11-18: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-11-19: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-11-20: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-11-21: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-11-22: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-11-25: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-11-26: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-11-27: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-11-29: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-12-02: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-12-03: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-12-04: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-12-05: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-12-06: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-12-09: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-12-10: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-12-11: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-12-12: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-12-13: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-12-16: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-12-17: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-12-18: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-12-19: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-12-20: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-12-23: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-12-24: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-12-26: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-12-27: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-12-30: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2024-12-31: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-01-02: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-01-03: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-01-06: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-01-07: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-01-08: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-01-10: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-01-13: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-01-14: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-01-15: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-01-16: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-01-17: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-01-21: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-01-22: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-01-23: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-01-24: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-01-27: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-01-28: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-01-29: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-01-30: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-01-31: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-02-03: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-02-04: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-02-05: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-02-06: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-02-07: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-02-10: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-02-11: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-02-12: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-02-13: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-02-14: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-02-18: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-02-19: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-02-20: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-02-21: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-02-24: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-02-25: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-02-26: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-02-27: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-02-28: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-03-03: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-03-04: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-03-05: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-03-06: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-03-07: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-03-10: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-03-11: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-03-12: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-03-13: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n |    |-- 2025-03-14: struct (nullable = true)\n |    |    |-- 1. open: string (nullable = true)\n |    |    |-- 2. high: string (nullable = true)\n |    |    |-- 3. low: string (nullable = true)\n |    |    |-- 4. close: string (nullable = true)\n |    |    |-- 5. volume: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a957890-d497-41f9-8be6-2a6590ac7947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee148ffe-1f30-4065-b3a8-b4f95d8f8abc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------------------------------------+\n|date      |values                                            |\n+----------+--------------------------------------------------+\n|2024-10-18|{236.1800, 236.1800, 234.0100, 235.0000, 46431472}|\n|2024-10-21|{234.4500, 236.8500, 234.4500, 236.4800, 36254470}|\n|2024-10-22|{233.8850, 236.2200, 232.6000, 235.8600, 38846578}|\n|2024-10-23|{234.0800, 235.1440, 227.7600, 230.7600, 52286979}|\n|2024-10-24|{229.9800, 230.8200, 228.4100, 230.5700, 31109503}|\n+----------+--------------------------------------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# Create a DataFrame with date and values as separate columns\n",
    "time_series_df = raw_df.select(\"Time Series (Daily).*\")\n",
    "date_columns = time_series_df.columns\n",
    "rows = []\n",
    "for date in date_columns:\n",
    "    rows.append(\n",
    "        time_series_df.select(\n",
    "            lit(date).alias(\"date\"),\n",
    "            col(date).alias(\"values\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Union all rows into one DataFrame\n",
    "flattened_df = rows[0]\n",
    "for row_df in rows[1:]:\n",
    "    flattened_df = flattened_df.union(row_df)\n",
    "\n",
    "flattened_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf6d4f09-dbcc-4da1-97e3-68294743812d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+------+------+--------+-----------+\n|date      |open   |high   |low   |close |volume  |daily_range|\n+----------+-------+-------+------+------+--------+-----------+\n|2024-10-18|236.18 |236.18 |234.01|235.0 |46431472|2.1699982  |\n|2024-10-21|234.45 |236.85 |234.45|236.48|36254470|2.4000092  |\n|2024-10-22|233.885|236.22 |232.6 |235.86|38846578|3.619995   |\n|2024-10-23|234.08 |235.144|227.76|230.76|52286979|7.3840027  |\n|2024-10-24|229.98 |230.82 |228.41|230.57|31109503|2.4100037  |\n+----------+-------+-------+------+------+--------+-----------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "clean_df = flattened_df.select(\n",
    "    col(\"date\"),\n",
    "    col(\"values.`1. open`\").cast(\"float\").alias(\"open\"),\n",
    "    col(\"values.`2. high`\").cast(\"float\").alias(\"high\"),\n",
    "    col(\"values.`3. low`\").cast(\"float\").alias(\"low\"),\n",
    "    col(\"values.`4. close`\").cast(\"float\").alias(\"close\"),\n",
    "    col(\"values.`5. volume`\").cast(\"long\").alias(\"volume\")\n",
    ")\n",
    "\n",
    "# Add a simple transformation\n",
    "clean_df = clean_df.withColumn(\"daily_range\", col(\"high\") - col(\"low\"))\n",
    "\n",
    "clean_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44c618eb-ed21-4051-a54e-9e6b6271f43e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Upload back to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5dcb024-c1da-48a6-a20c-6f802bb1cb03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+------+------+--------+-----------+\n|date      |open   |high   |low   |close |volume  |daily_range|\n+----------+-------+-------+------+------+--------+-----------+\n|2024-10-18|236.18 |236.18 |234.01|235.0 |46431472|2.1699982  |\n|2024-10-21|234.45 |236.85 |234.45|236.48|36254470|2.4000092  |\n|2024-10-22|233.885|236.22 |232.6 |235.86|38846578|3.619995   |\n|2024-10-23|234.08 |235.144|227.76|230.76|52286979|7.3840027  |\n|2024-10-24|229.98 |230.82 |228.41|230.57|31109503|2.4100037  |\n+----------+-------+-------+------+------+--------+-----------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "file_key = \"processed/data/AAPL.parquet\"\n",
    "output_s3_path = f\"s3://{bucket_name}/{file_key}\"\n",
    "clean_df.write.mode(\"overwrite\").parquet(output_s3_path)\n",
    "\n",
    "verify_df = spark.read.parquet(output_s3_path)\n",
    "verify_df.show(5, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7513946077669852,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "s3_integration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}